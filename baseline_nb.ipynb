{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [open in Kaggle](https://www.kaggle.com/meishidou/baseline-nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "# package in . directory\n",
    "from bichoice import utils\n",
    "from baseline.model import BertForClassification\n",
    "from baseline.data_processor import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert transformers.__version__ == '4.1.1'\n",
    "\n",
    "# declare a namespace\n",
    "D = utils.GlobalSettings({\n",
    "        'DATADIR': './data/',\n",
    "        # select checkpoint-7\n",
    "        'MODELDIR': './outputs/baseline_output/checkpoint-7/',\n",
    "    })\n",
    "\n",
    "# load training parameters\n",
    "argD = utils.GlobalSettings(\n",
    "    torch.load(os.path.join(D.MODELDIR, 'training_args.bin')))\n",
    "print('this model is trained with following hyper parameters:')\n",
    "print(str(argD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(D.DATADIR)\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(argD.model_name)\n",
    "train = processor.dataset[0]\n",
    "# select a literal example from train set\n",
    "train_e = random.choice(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_baseline_example(e):\n",
    "    '''show all info of a single `baseline.InputExample` object`'''\n",
    "    print('text_a:')\n",
    "    print('    ', e.text_a)\n",
    "    print('text_b:')\n",
    "    print('    ', e.text_b)\n",
    "    print('text_c:')\n",
    "    print('    ', e.text_c)\n",
    "    print('label:', e.label)\n",
    "    print('guid:', e.guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create several baseline examples from `train_e`\n",
    "train_b_e = processor._create_examples([train_e], set_type='train')\n",
    "for i, e in enumerate(train_b_e):\n",
    "    print('-----EXAMPLE{}-----'.format(i+1))\n",
    "    show_baseline_example(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_baseline_features(f):\n",
    "    '''show info of a single `baseline.InputFeatures` object'''\n",
    "    print('-----FIRST TOKEN SEQUENCE-----')\n",
    "    input_mask = np.asarray(f.input_mask)\n",
    "    input_ids = np.asarray(f.input_ids)[input_mask==1]\n",
    "    segment_ids = np.asarray(f.segment_ids)[input_mask==1]\n",
    "    first_sent = tokenizer.convert_ids_to_tokens(input_ids[segment_ids==0])\n",
    "    second_sent = tokenizer.convert_ids_to_tokens(input_ids[segment_ids==1])\n",
    "    print(''.join(first_sent))\n",
    "    print('-----SECOND TOKEN SEQUENCE-----')\n",
    "    print(''.join(second_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f_e = processor.convert_examples_to_features(\n",
    "    train_b_e, argD.max_length, tokenizer)[0]\n",
    "print('label:', train_b_e[0].label)\n",
    "for i, f in enumerate(train_f_e):\n",
    "    print('-----EXAMPLE{}-----'.format(i+1))\n",
    "    show_baseline_features(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer with Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and load state dict from a checkpoint \n",
    "device = 'cuda:0' # not compatible with cpu\n",
    "model = BertForClassification(argD.model_name)\n",
    "model.load_state_dict(torch.load(os.path.join(D.MODELDIR, 'model.bin')))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = processor.get_dataset(train_b_e, tokenizer, argD.max_length)[:]\n",
    "b = tuple(t.to(device) for t in b)\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=b[0], \n",
    "                   attention_mask=b[1], \n",
    "                   token_type_ids=b[2], \n",
    "                   labels=b[3])\n",
    "logits = output[1].detach().cpu().numpy()\n",
    "pred = np.argmax(logits, axis=1)[0]\n",
    "label = b[3][0]\n",
    "options = [e.text_b for e in train_b_e]\n",
    "print('infered answer:', options[pred])\n",
    "print('correct answer:', options[label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
